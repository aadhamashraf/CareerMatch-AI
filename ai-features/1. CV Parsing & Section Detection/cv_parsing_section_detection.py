# -*- coding: utf-8 -*-
"""cv_parsing_section_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxq_7nt_lqeRBXFcusb6SaM-NRZFkvE6
"""

!pip install pdfplumber docx2txt transformers torch scikit-learn

import pdfplumber
import docx2txt

def extract_text_from_file(file_path):
    if file_path.endswith(".pdf"):
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
    elif file_path.endswith(".docx"):
        text = docx2txt.process(file_path)
    else:
        raise ValueError("Unsupported file format. Please upload PDF or DOCX.")
    return text

import re

def split_text_into_sentences(text):
    sentences = re.split(r'\n+|(?<=[.!?])\s+', text.strip())
    return [s.strip() for s in sentences if len(s.strip()) > 10]

# from transformers import pipeline

# classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# labels = ["Education", "Experience", "Projects", "Skills", "Certifications"]

# def classify_cv_text(sentences):
#     results = []
#     for sentence in sentences:
#         pred = classifier(sentence, labels)
#         label = pred["labels"][0]
#         score = pred["scores"][0]
#         results.append({"text": sentence, "category": label, "confidence": round(score, 3)})
#     return results

file_path = "/content/Adham_Ashraf_Eltholth.pdf"
text = extract_text_from_file(file_path)
sentences = split_text_into_sentences(text)

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import pandas as pd


df = pd.read_csv("/content/technical_cv_augmented_ollama.csv")

label2id = {label: idx for idx, label in enumerate(df["label"].unique())}
id2label = {idx: label for label, idx in label2id.items()}

df["label"] = df["label"].map(label2id)

dataset_hf = Dataset.from_pandas(df)

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_fn(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

dataset_hf = dataset_hf.map(tokenize_fn, batched=True)
dataset_hf = dataset_hf.rename_column("label", "labels")
dataset_hf.set_format("torch")

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

args = TrainingArguments(
    output_dir="cv-section-classifier",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    do_eval=True,   # old-style flag to enable evaluation
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset_hf,
    eval_dataset=dataset_hf,
)

trainer.train()

