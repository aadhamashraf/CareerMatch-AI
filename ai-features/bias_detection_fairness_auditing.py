# -*- coding: utf-8 -*-
"""Bias Detection / Fairness Auditing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jiC0ecLkz-KjYO9S86usNINT_0_Ceb6l
"""

import io, json, math, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from typing import Dict, List, Optional, Tuple
from sklearn.metrics import confusion_matrix, brier_score_loss, roc_curve

warnings.filterwarnings("ignore")

try:
    from google.colab import files
    COLAB = True
except Exception:
    COLAB = False

df = None
if COLAB:
    print("Upload a CSV with columns: candidate_id, score, [label], gender, education_tier, background")
    up = files.upload()
    if up:
        fname = list(up.keys())[0]
        df = pd.read_csv(io.BytesIO(up[fname]))
        print("Loaded:", fname, df.shape)

if df is None:
    # Synthetic demo (so notebook runs without a file)
    rng = np.random.default_rng(7)
    n = 400
    df = pd.DataFrame({
        "candidate_id": np.arange(n),
        "score": np.clip(rng.normal(65, 12, n), 0, 100),
        "gender": rng.choice(["male","female","nonbinary"], n, p=[0.5,0.47,0.03]),
        "education_tier": rng.choice(["top_tier","regular","bootcamp","self_taught"], n, p=[0.25,0.55,0.1,0.1]),
        "background": rng.choice(["traditional","non_traditional"], n, p=[0.7, 0.3]),
    })
    # pseudo-label from score (for equality/calibration metrics)
    logits = (df["score"] - 60)/8
    probs = 1 / (1 + np.exp(-logits))
    df["label"] = (rng.random(n) < probs).astype(int)
    print("Loaded synthetic data:", df.shape)

# Normalize columns
df.columns = [c.strip().lower() for c in df.columns]

# 1) Config
# ------------------------------
SCORE_COL = "score"          # your Strength/compatibility score (0â€“100)
LABEL_COL = "label"          # optional (0/1 ground-truth or downstream outcome)
GROUP_COLS = ["gender", "education_tier", "background"]  # edit as needed

# "Positive" decision policy:
#   A) fixed threshold on score, OR
#   B) top-k selection
USE_TOP_K = False
SCORE_THRESHOLD = 70.0  # if not using top-k
TOP_K = 100             # if using top-k

# Disparate impact 80% rule bounds
DI_LOWER, DI_UPPER = 0.8, 1.25

# 2) Helper functions
# ------------------------------
def select_positive_mask(scores: pd.Series) -> pd.Series:
    if USE_TOP_K:
        k = min(TOP_K, len(scores))
        cutoff = scores.sort_values(ascending=False).iloc[k-1] if k > 0 else 1e9
        return scores >= cutoff
    return scores >= SCORE_THRESHOLD

def group_rates(y_pred: pd.Series, y_true: Optional[pd.Series], group: pd.Series) -> pd.DataFrame:
    """Compute selection rate and, if labels exist, TPR/FPR per group."""
    out = []
    for g, idx in group.groupby(group).groups.items():
        yp = y_pred.loc[idx]
        sel_rate = yp.mean() if len(yp) else np.nan
        row = {"group": g, "n": len(idx), "selection_rate": sel_rate}
        if y_true is not None and y_true.notna().any():
            yt = y_true.loc[idx].astype(int)
            # confusion matrix: tn, fp, fn, tp
            try:
                tn, fp, fn, tp = confusion_matrix(yt, yp.astype(int), labels=[0,1]).ravel()
            except ValueError:
                # If a class is missing in this slice, pad zeros
                tn=fp=fn=tp=0
                if yt.eq(0).all() and yp.eq(0).all(): tn=len(yt)
                if yt.eq(1).all() and yp.eq(1).all(): tp=len(yt)
            tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan
            fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan
            row.update({"tpr": tpr, "fpr": fpr})
        out.append(row)
    return pd.DataFrame(out).sort_values("group").reset_index(drop=True)

def disparate_impact(selection_rates: pd.Series, ref_rate: float) -> pd.Series:
    """Rate_group / Rate_ref (avoid div-by-zero)."""
    return selection_rates / (ref_rate if ref_rate > 0 else np.nan)

def brier_by_group(scores: pd.Series, labels: pd.Series, group: pd.Series) -> pd.DataFrame:
    """Calibration proxy using Brier score on (score/100) as probability."""
    out = []
    p = np.clip(scores / 100.0, 1e-6, 1-1e-6)
    for g, idx in group.groupby(group).groups.items():
        y = labels.loc[idx].astype(int)
        b = brier_score_loss(y, p.loc[idx]) if len(idx) else np.nan
        out.append({"group": g, "n": len(idx), "brier": b})
    return pd.DataFrame(out).sort_values("group").reset_index(drop=True)

def fairness_table(df: pd.DataFrame, group_col: str) -> Dict[str, pd.DataFrame]:
    """Compute metrics for a given protected attribute."""
    assert SCORE_COL in df, f"Missing {SCORE_COL}"
    y_pred = select_positive_mask(df[SCORE_COL])
    y_true = df[LABEL_COL] if LABEL_COL in df else None
    grp = df[group_col].astype(str)

    rates = group_rates(y_pred, y_true, grp)
    # reference = highest selection rate group (common choice) or pick a specific ref class
    ref_idx = rates["selection_rate"].idxmax()
    ref_group = rates.loc[ref_idx, "group"]
    ref_rate = rates.loc[ref_idx, "selection_rate"]

    rates["disparate_impact"] = disparate_impact(rates["selection_rate"], ref_rate)
    rates["stat_parity_diff"] = rates["selection_rate"] - ref_rate
    rates["flag_80pct_rule"] = ~rates["disparate_impact"].between(DI_LOWER, DI_UPPER)

    cal = None
    if LABEL_COL in df:
        cal = brier_by_group(df[SCORE_COL], df[LABEL_COL], grp)

    return {
        "group": group_col,
        "reference_group": pd.DataFrame([{"reference_group": ref_group, "reference_rate": ref_rate}]),
        "rates": rates,
        "calibration": cal
    }

# 3) Run audits
# ------------------------------
available_groups = [g for g in GROUP_COLS if g in df.columns]
reports = {}
for g in available_groups:
    reports[g] = fairness_table(df, g)

# 4) Display tables
# ------------------------------
for g in reports:
    print("\n==============================")
    print(f"FAIRNESS REPORT by '{g}'")
    print("==============================")
    display(reports[g]["reference_group"])
    display(reports[g]["rates"])
    if reports[g]["calibration"] is not None:
        print("Calibration (Brier score, lower is better):")
        display(reports[g]["calibration"])

# 5) Plots (selection rate + score distribution)
# ------------------------------
def plot_selection_rates(df: pd.DataFrame, group_col: str):
    y_pred = select_positive_mask(df[SCORE_COL])
    plot = (
        df.assign(pred=y_pred)
          .groupby(group_col)["pred"]
          .mean()
          .sort_values(ascending=False)
          .rename("selection_rate")
          .to_frame()
    )
    ax = plot.plot(kind="bar", figsize=(6,3))
    ax.set_ylim(0,1)
    ax.set_title(f"Selection Rate by {group_col}")
    ax.grid(True, axis="y")
    plt.show()

def plot_score_hist(df: pd.DataFrame, group_col: str):
    plt.figure(figsize=(7,4))
    for g, sub in df.groupby(group_col):
        # Skip plotting KDE for groups with only one member
        if len(sub) > 1:
            sub[SCORE_COL].plot(kind="kde", label=str(g))
        else:
            print(f"Skipping KDE plot for group '{g}' in column '{group_col}' as it has only {len(sub)} member.")
    plt.title(f"Score Distribution by {group_col}")
    plt.xlabel("Score")
    plt.legend()
    plt.grid(True, axis="y")
    plt.show()

for g in available_groups:
    plot_selection_rates(df, g)
    plot_score_hist(df, g)

# 6) Build a JSON + CSV report
# ------------------------------
def serialize_reports(reports: Dict[str, Dict]) -> Dict:
    out = {"policy": {
                "use_top_k": USE_TOP_K,
                "score_threshold": SCORE_THRESHOLD,
                "top_k": TOP_K,
                "score_column": SCORE_COL,
                "label_column": LABEL_COL if LABEL_COL in df.columns else None,
                "di_bounds": [DI_LOWER, DI_UPPER],
            },
           "audits": {}}
    for g, rep in reports.items():
        ref = rep["reference_group"].to_dict(orient="records")[0]
        rates = rep["rates"].to_dict(orient="records")
        cal = rep["calibration"].to_dict(orient="records") if rep["calibration"] is not None else None
        out["audits"][g] = {"reference_group": ref, "rates": rates, "calibration": cal}
    return out

report_json = serialize_reports(reports)

# Flags summary (80% rule)
flags = []
for g, rep in reports.items():
    r = rep["rates"]
    bad = r[r["flag_80pct_rule"] == True]
    if len(bad):
        for _, row in bad.iterrows():
            flags.append({"group": g,
                          "value": row["group"],
                          "disparate_impact": round(float(row["disparate_impact"]), 3)})
flags_df = pd.DataFrame(flags)
print("\n=== Flags (violating 80% rule) ===")
display(flags_df if len(flags_df) else pd.DataFrame([{"status": "No DI violations"}]))

# Save artifacts
pd.DataFrame(report_json["audits"][available_groups[0]]["rates"]).to_csv("fairness_rates_sample.csv", index=False)
with open("fairness_report.json", "w", encoding="utf-8") as f:
    json.dump(report_json, f, indent=2)
print("Saved: fairness_report.json, fairness_rates_sample.csv")

# 7) Hints for mitigation
# ------------------------------
def mitigation_hints():
    tips = [
        "- Review feature set for proxies of protected attributes; drop/transform if needed.",
        "- Consider **group-aware thresholding** (tune cutoffs to equalize TPR or selection rates within acceptable bands).",
        "- Use **post-processing** methods like reject-option classification to nudge borderline decisions fairly.",
        "- Try **re-weighting** or **re-sampling** training data to balance representation.",
        "- Add **explainability**: log per-candidate reasons to detect systematic gaps (e.g., projects count vs. education).",
        "- Monitor over time: run this audit per batch/week and store metrics to catch drift."
    ]
    return "\n".join(tips)

print("\n=== Mitigation Hints ===\n" + mitigation_hints())