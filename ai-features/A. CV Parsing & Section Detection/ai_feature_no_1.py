# -*- coding: utf-8 -*-
"""ai feature no 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFPsx3BCScZ1spOwI-Lnwr-38yMq8sc0

# Career-Match — Minimal CV Compatibility Scorer (S, R, C)

**Inputs (only two):**
1. **CV JSON** (upload)
2. **Job Title** (optional: description & essential skills)

**Outputs:**
- **Strength (S)** — 0–100 (weighted sum of Education, Experience, Projects, Skills, Certifications)
- **Relevance (R)** — 0–100 (cosine similarity between CV text and job text)
- **Completeness (C)** — 0–100 (% of essential skills present)

> You can extend this later with sentence embeddings and learned weights.
"""

# If running on Colab, install dependencies if needed:
# !pip install scikit-learn

import json, re
from typing import List, Dict

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Detect if running on Google Colab
try:
    from google.colab import files
    COLAB = True
except Exception:
    COLAB = False

"""
## 1) Upload CV JSON
- On Colab, run the next cell and choose your `cv_parsed.json`.
- Required fields are flexible. Minimal helpful fields:
  - `summary` (str), `skills` (list or dict), `projects` (list), `experience`, `education`, `certifications`.
"""

cv = None
if COLAB:
    print("Upload your CV JSON (e.g., cv_parsed.json)")
    up = files.upload()
    if len(up) > 0:
        fname = list(up.keys())[0]
        with open(fname, "r", encoding="utf-8") as f:
            cv = json.load(f)
        print("Loaded CV from:", fname)
else:
    # If not on Colab, you can hardcode a path or load a sample here.
    # Example fallback (edit path if necessary):
    try:
        with open("/mnt/data/cv_parsed.json", "r", encoding="utf-8") as f:
            cv = json.load(f)
        print("Loaded CV from /mnt/data/cv_parsed.json")
    except Exception as e:
        print("No CV file loaded. Please upload in Colab or set a valid path.")
        raise

assert isinstance(cv, dict), "CV JSON must be a dict-like object."
print("CV keys:", list(cv.keys())[:12])

"""
## 2) Enter Job Title (and optional details)

- At minimum, set **job_title**.  
- Optionally, set **job_description** and **essential_skills**.  
- If you **leave essential_skills empty**, we'll infer a small default list from the title (best effort).
"""

# === EDIT ME ===
job_title = "Machine Learning Engineer"

# Optional details (recommended for better scoring)
job_description = (
    "Design and deploy machine learning systems. Strong Python, pandas, numpy, "
    "scikit-learn, deep learning (TensorFlow/PyTorch), APIs (FastAPI/Flask), Docker, and cloud (AWS/GCP)."
)

# If you already know the essentials, list them here. Otherwise leave empty [] and the code will try to infer.
essential_skills = []  # e.g., ["Python", "Pandas", "NumPy", "Scikit-Learn", "TensorFlow", "SQL", "Docker", "AWS", "FastAPI"]

# Simple inference of essential skills given job_title, if user left it empty.
# This is a tiny built-in mapping for convenience. Extend as needed.
DEFAULT_SKILLS_BY_TITLE = {
    "data scientist": ["Python","Pandas","NumPy","Scikit-Learn","SQL","Data Visualization","Statistics","ML Ops"],
    "machine learning engineer": ["Python","Pandas","NumPy","Scikit-Learn","TensorFlow","PyTorch","Docker","AWS","FastAPI","SQL"],
    "deep learning engineer": ["Python","PyTorch","TensorFlow","NumPy","Computer Vision","NLP","CUDA","Docker","AWS"],
    "backend developer": ["Python","Django","FastAPI","PostgreSQL","Docker","CI/CD","AWS"],
    "frontend developer": ["JavaScript","React","TypeScript","HTML","CSS","REST","Testing"],
    "data analyst": ["SQL","Excel","Python","Pandas","Data Visualization","Power BI","Tableau"],
}

def infer_essential_skills(title: str) -> List[str]:
    t = (title or "").strip().lower()
    for k, v in DEFAULT_SKILLS_BY_TITLE.items():
        if k in t:
            return v
    # Generic fallback
    return ["Python","SQL","Git"]

if not essential_skills:
    essential_skills = infer_essential_skills(job_title)

print("Job Title:", job_title)
print("Essential Skills ({}):".format(len(essential_skills)), essential_skills)
print("Job Description present:", bool(job_description.strip()))

"""
## 3) Scoring Functions (S, R, C)
"""

def normalize_token(token: str) -> str:
    import re
    return re.sub(r"[^a-z0-9#+]+", "", token.lower())

def cv_to_text(cv_obj: Dict) -> str:
    parts = []
    if cv_obj.get("summary"):
        parts.append(str(cv_obj["summary"]))

    # Skills can be dict or list
    sk = cv_obj.get("skills")
    if isinstance(sk, dict):
        for v in sk.values():
            if isinstance(v, list):
                parts.append(" ".join(map(str, v)))
            elif isinstance(v, str):
                parts.append(v)
    elif isinstance(sk, list):
        parts.append(" ".join(map(str, sk)))

    # Projects list: accept dicts or strings
    for p in cv_obj.get("projects", []) or []:
        if isinstance(p, dict):
            if p.get("name"): parts.append(str(p["name"]))
            if p.get("description"): parts.append(str(p["description"]))
            if p.get("tech"):
                if isinstance(p["tech"], list): parts.append(" ".join(map(str, p["tech"])))
                else: parts.append(str(p["tech"]))
        else:
            parts.append(str(p))

    # Experience, education, certifications: stringify
    for key in ["experience", "education", "certifications"]:
        if key in cv_obj and cv_obj[key]:
            parts.append(str(cv_obj[key]))

    return "\n".join(parts)

def collect_skill_tokens(cv_obj: Dict) -> set:
    tokens = set()
    sk = cv_obj.get("skills")
    if isinstance(sk, dict):
        for v in sk.values():
            if isinstance(v, list):
                for s in v: tokens.add(normalize_token(str(s)))
            elif isinstance(v, str):
                tokens.add(normalize_token(v))
    elif isinstance(sk, list):
        for s in sk: tokens.add(normalize_token(str(s)))

    text = cv_to_text(cv_obj)
    for s in re.split(r"[,\s/()\-]+", text):
        if s: tokens.add(normalize_token(s))
    return tokens

def completeness_score(cv_obj: Dict, essential_skills: List[str]) -> float:
    req = [normalize_token(s) for s in (essential_skills or [])]
    if not req: return 0.0
    have = collect_skill_tokens(cv_obj)
    present = sum(1 for s in req if s in have)
    return (present / len(req)) * 100.0

def relevance_score(cv_obj: Dict, job_text: str) -> float:
    cv_text = cv_to_text(cv_obj).strip()
    job_text = (job_text or "").strip()
    if not cv_text or not job_text:
        return 0.0
    vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)
    X = vec.fit_transform([cv_text, job_text])
    return float(cosine_similarity(X[0], X[1])[0][0] * 100.0)

def presence_score_any(cv_obj: Dict, key: str) -> float:
    val = cv_obj.get(key)
    if not val: return 0.0
    if isinstance(val, (list, dict)): return 1.0 if len(val) > 0 else 0.0
    return 1.0

def education_proxy(cv_obj: Dict) -> float:
    if presence_score_any(cv_obj, "education") > 0: return 1.0
    summary = cv_obj.get("summary", "") or ""
    return 0.7 if re.search(r"\b(bachelor|undergraduate|bsc|bs|ba|msc|master|phd)\b", summary.lower()) else 0.0

def experience_proxy(cv_obj: Dict) -> float:
    return presence_score_any(cv_obj, "experience")

def projects_proxy(cv_obj: Dict) -> float:
    projects = cv_obj.get("projects", []) or []
    return min(1.0, (len(projects) if isinstance(projects, list) else 1) / 5.0)

def skills_proxy(cv_obj: Dict, essential_skills: List[str]) -> float:
    return completeness_score(cv_obj, essential_skills) / 100.0

def certifications_proxy(cv_obj: Dict) -> float:
    return presence_score_any(cv_obj, "certifications")

def strength_score(cv_obj: Dict, essential_skills: List[str], weights: Dict[str, float]) -> Dict[str, float]:
    Edu  = education_proxy(cv_obj)
    Exp  = experience_proxy(cv_obj)
    Proj = projects_proxy(cv_obj)
    Skills = skills_proxy(cv_obj, essential_skills)
    Cert = certifications_proxy(cv_obj)
    wsum = sum(weights.values()) or 1.0
    w = {k: v/wsum for k, v in weights.items()}
    S = 100.0 * (w["Edu"]*Edu + w["Exp"]*Exp + w["Proj"]*Proj + w["Skills"]*Skills + w["Cert"]*Cert)
    return {"S": float(S), "Edu": float(Edu), "Exp": float(Exp), "Proj": float(Proj), "Skills": float(Skills), "Cert": float(Cert)}

"""
## 4) Configure Strength Weights (S)
"""

# You can edit these. They will be normalized to sum to 1.
weights = {"Edu": 0.2, "Exp": 0.25, "Proj": 0.2, "Skills": 0.25, "Cert": 0.1}
weights

"""
## 5) Compute Scores
"""

# Build job_text for Relevance (R). Prefer description; fallback to title.
job_text = job_description if job_description and job_description.strip() else job_title

C = completeness_score(cv, essential_skills)
R = relevance_score(cv, job_text)
S_detail = strength_score(cv, essential_skills, weights)

result = {
    "job_profile": job_title,
    "scores": {
        "Strength_S": round(S_detail["S"], 2),
        "Relevance_R": round(R, 2),
        "Completeness_C": round(C, 2)
    },
    "subscores": {
        "Education": round(S_detail["Edu"], 2),
        "Experience": round(S_detail["Exp"], 2),
        "Projects": round(S_detail["Proj"], 2),
        "Skills": round(S_detail["Skills"], 2),
        "Certifications": round(S_detail["Cert"], 2)
    }
}

print("=== Scores ===")
print("Strength (S):   ", result["scores"]["Strength_S"])
print("Relevance (R):  ", result["scores"]["Relevance_R"])
print("Completeness (C):", result["scores"]["Completeness_C"])

print("\n--- Subscores (0–1) ---")
for k in ["Education","Experience","Projects","Skills","Certifications"]:
    print(f"{k:>13}: {result['subscores'][k]}")

# Also keep the structured dict available for programmatic use
result